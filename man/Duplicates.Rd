% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Duplicates_class.R
\docType{class}
\name{Duplicates-class}
\alias{Duplicates-class}
\alias{Duplicates}
\title{Detect Duplicates}
\arguments{
\item{x}{a \code{"partitionBundle"} object defining the documents that will be compared to detect duplicates}

\item{charRegex}{a regex defining the characters to keep}

\item{sAttribute}{the s-attribute providing the date}

\item{sample}{number of documents to define a subset of \code{partitionBundle} to speed up character count}

\item{n}{number of days before and after a document was published}

\item{threshold}{numeric (0 < x < 1), the minimum similarity to qualify two documents as duplicates}

\item{mc}{logical, whether to use multicore}

\item{verbose}{logical, whether to be verbose}

\item{progress}{logical, whether to show progress bar}
}
\description{
Class for duplicate detection.
}
\details{
The class implements a procedure described by Fritz Kliche, Andre Blessing,
Urlich Heid and Jonathan Sonntag in the paper "The eIdentity Text
ExplorationWorkbench" presented at LREC 2014
(see \url{http://www.lrec-conf.org/proceedings/lrec2014/pdf/332_Paper.pdf}).

To detect duplicates, choices are made as follows:
(a) If two similar articles have been published on the same day, the shorter article will
be considered the duplicate; (b) if two similar articles were published on different days,
the article that appeared later will be considered the duplicate.

Different \code{partitionBundle}-objects can be passed into the \code{detectDuplicates}-method successively. The field
\code{duplicates} will be appended by the duplicates that are newly detected.
}
\section{Fields}{

\describe{
\item{\code{corpus}}{the CWB corpus the (last) \code{partitionBundle} used describes}

\item{\code{charRegex}}{regex defining the characters to keep}

\item{\code{charCount}}{count of the characters in the \code{partitionBundle}}

\item{\code{number}}{of days before and after a document was published}

\item{\code{pAttribute}}{the p-attribute used (defaults to "word")}

\item{\code{sAttribute}}{the s-attribute of the date of a text in the corpus}

\item{\code{sample}}{size of the sample of the \code{partitionBundle} that the character count is based on}

\item{\code{threshold}}{minimum similarity value to identify two texts as duplicates}

\item{\code{whatToCompare}}{a \code{simple_triplet_matrix} with the texts to be compared}

\item{\code{similarityMatrix}}{a \code{simple_triplet_matrix} with similarities of texts}

\item{\code{ngramDocumentMatrix}}{a matrix (inheriting from \code{TermDocumentMatrix}) with ngram counts in the documents of the \code{partitionBundle}}

\item{\code{datePrep}}{function to rework dates if not in the DD-MM-YYYY standard format}

\item{\code{annotation}}{a \code{data.table} with corpus positions}
}}

\section{Methods}{

\describe{
\item{\code{detectDuplicates(x, verbose = TRUE, mc = FALSE, progress = TRUE)}}{Wrapper that implements the entire workflow for duplicate detection.}

\item{\code{encode(exec = FALSE, filenames = list(duplicate = tempfile(), original =
  tempfile()))}}{Add structural attributes to CWB corpus based on the annotation data that has been generated
(data.table in field annotation).}

\item{\code{getWhatToCompare(x, reduce = TRUE, verbose = FALSE, progress = TRUE,
  mc = FALSE)}}{Identify documents that will be compared (based on date of documents).}

\item{\code{initialize(charRegex = "[a-zA-Z]", pAttribute = "word",
  sAttribute = "text_date", datePrep = NULL, sample = 1000L, n = 1L,
  threshold = 0.9)}}{Initialize object of class 'Duplicates'.}

\item{\code{makeAnnotation(sAttributeID)}}{Turn data.table with duplicates into file with corpus positions and annotation of duplicates,
generate cwb-s-encode command and execute it, if wanted.}

\item{\code{makeDuplicateDataTable(x, mc = FALSE, progress = TRUE, verbose = TRUE)}}{Turn similarities of documents into a data.table that identifies original document and duplicate.}
}}

\examples{
\dontrun{
foo <- partitionBundle(
  "KEYWORDS",
  def = list(text_newspaper="guardian"),
  var=list(text_id=sAttributes("KEYWORDS", "text_id")[1:500]),
  pAttribute=NULL
 )
doubled <- duplicates(foo)
}
}
